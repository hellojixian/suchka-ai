{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-13 23:23:25.779097: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-13 23:23:25.800057: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-13 23:23:27.557327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20548 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "2023-06-13 23:23:27.557655: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 20550 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:09:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname('../')))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "from deepface.basemodels import VGGFace\n",
    "tf_face = VGGFace.loadModel()\n",
    "\n",
    "from deepface.extendedmodels import Gender\n",
    "tf_gender = Gender.loadModel()\n",
    "\n",
    "from core.face.gender import Gender as PTGender\n",
    "pt_gender = PTGender()\n",
    "\n",
    "from core.face.vggface import VGGFace as PTVGGFace\n",
    "torch_model = PTVGGFace()\n",
    "# import torch\n",
    "# from core.face.vggface import VGGFace as PTVGGFace\n",
    "# torch_model = PTVGGFace()\n",
    "\n",
    "# weights_file = os.path.join(project_root, \"pretrained/vggface.pth\")\n",
    "# torch_model.load_state_dict(torch.load(weights_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 39\n",
      "(7, 7, 512, 4096) (7, 7, 512, 4096)\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x7fe1e3936020>\n",
      "(7, 7, 512, 4096) torch.Size([4096, 512, 7, 7])\n",
      "[-1.3083947e-04 -1.8606588e-05 -3.1403993e-05 -2.5138357e-05\n",
      "  5.8494537e-05 -1.4254390e-03 -3.2078020e-05 -1.1948898e-03\n",
      " -1.5005474e-03 -1.1764151e-03]\n",
      "[-4.2590671e-03 -1.8606588e-05 -1.1356508e-03 -2.5138357e-05\n",
      "  5.8494537e-05 -1.4254390e-03 -3.2078020e-05 -1.9952398e-02\n",
      " -1.5005474e-03 -1.1306326e-02]\n",
      "------------------\n",
      "(1, 1, 4096, 4096) (1, 1, 4096, 4096)\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x7fe1e3907fd0>\n",
      "(1, 1, 4096, 4096) torch.Size([4096, 4096, 1, 1])\n",
      "[-7.9717307e-04  1.7204320e-04  3.0277763e-04 -1.1735294e-03\n",
      "  1.1266565e-03  4.2761283e-04  5.6686375e-04 -1.0834980e-03\n",
      "  8.9018205e-05 -2.5601126e-04]\n",
      "[-0.00585147  0.00172603 -0.00489362 -0.00117353  0.00112666  0.00042761\n",
      "  0.00056686  0.00153397 -0.00501941 -0.00025601]\n",
      "------------------\n",
      "(1, 1, 4096, 2622) (1, 1, 4096, 2)\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x7fe1e394ca90>\n",
      "(1, 1, 4096, 2) torch.Size([2, 4096, 1, 1])\n",
      "[-0.01183689 -0.00843864 -0.00565795 -0.00274284 -0.01450934 -0.0028405\n",
      " -0.00491976 -0.00419261  0.00520571 -0.00645062]\n",
      "[ 0.01046555  0.01595668 -0.00032442 -0.00970924 -0.00130117  0.02422934\n",
      " -0.01682499 -0.00604054 -0.00161014  0.02545892]\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "print(len(tf_face.layers), len(tf_gender.layers))\n",
    "\n",
    "tf_gender_layer_id = 0\n",
    "for i in range(len(tf_gender.layers)):  \n",
    "  face_layer = tf_face.layers[i] if i < len(tf_face.layers) else None\n",
    "  gender_layer =  tf_gender.layers[i]\n",
    "  if face_layer: \n",
    "    pass\n",
    "  #   print(face_layer.name, gender_layer.name)\n",
    "  else:\n",
    "  #   print(gender_layer.name)\n",
    "    continue\n",
    "  \n",
    "  if not gender_layer.get_weights(): continue\n",
    "  flatten_face_weights = face_layer.get_weights()[0].reshape(-1)\n",
    "  flatten_gender_weights = gender_layer.get_weights()[0].reshape(-1)\n",
    "  \n",
    "  if not np.array_equal(flatten_face_weights, flatten_gender_weights):    \n",
    "    print(face_layer.get_weights()[0].shape, gender_layer.get_weights()[0].shape)\n",
    "    pt_gender_layer_id = 0\n",
    "    for pt_gender_layer in pt_gender.modules():\n",
    "      if isinstance(pt_gender_layer, torch.nn.Conv2d):\n",
    "        if gender_layer.get_weights():\n",
    "          if (pt_gender_layer_id == tf_gender_layer_id):\n",
    "            weights = gender_layer.get_weights()\n",
    "            pt_gender_layer.weight.data = torch.from_numpy(weights[0].transpose(3, 2, 0, 1))\n",
    "            pt_gender_layer.bias.data = torch.from_numpy(weights[1])\n",
    "            print(gender_layer)\n",
    "            print(gender_layer.get_weights()[0].shape, pt_gender_layer.weight.data.shape)\n",
    "            print(flatten_face_weights[:10])\n",
    "            print(flatten_gender_weights[:10])\n",
    "            print('------------------')\n",
    "        pt_gender_layer_id += 1      \n",
    "    tf_gender_layer_id += 1          \n",
    "    \n",
    "torch.save(pt_gender.state_dict(), os.path.join(project_root, \"pretrained/gender.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9999594688415527, 4.055035242345184e-05]\n",
      "[0.9999594688415527, 4.055035242345184e-05]\n",
      "pt_gender: [0.9999594688415527, 4.0530670958105475e-05]\n",
      "female\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "test_img = '/home/jixian/tmp/face_0_cropped.jpg'\n",
    "test_img = '/home/jixian/tmp/3e6469e1b29ffa4e399cdd3b4e974a71914ca34c.jpg'\n",
    "img = cv2.imread(test_img)\n",
    "img = cv2.resize(img, (224, 224))\n",
    "tf_img = np.expand_dims(img, axis=0)\n",
    "tf_img = tf_img.astype(np.float32)\n",
    "tf_img /= 255.0\n",
    "tf_res = tf_gender.predict(tf_img, verbose=0)[0].tolist()\n",
    "print(tf_res)\n",
    "tf_res = tf_gender.predict(tf_img, verbose=0)[0].tolist()\n",
    "print(tf_res)\n",
    "# convert img to torch tensor\n",
    "\n",
    "img = cv2.imread(test_img)\n",
    "img = cv2.resize(img, (224, 224))\n",
    "img = img.astype(np.float32)\n",
    "img /= 255.0\n",
    "img = img.transpose(2, 0, 1)\n",
    "torch_img = torch.from_numpy(img)\n",
    "\n",
    "pt_gender.eval()\n",
    "with torch.no_grad():  \n",
    "  features = torch_model.features(torch_img)\n",
    "  # print(features.shape, features[:10])\n",
    "  gender_probs = pt_gender(features)\n",
    "  torch_res = gender_probs.detach().numpy().tolist()\n",
    "  print('pt_gender:',torch_res)\n",
    "    \n",
    "  torch_res = pt_gender.predict(gender_probs)\n",
    "  print(torch_res)\n",
    "\n",
    "#   torch_res2 = torch_model(torch_img).reshape(-1).detach().numpy().tolist()\n",
    "#   print(torch_res2[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 119554050\n",
      "torch.Size([4096, 512, 7, 7]) 102760448\n",
      "torch.Size([4096]) 4096\n",
      "torch.Size([4096, 4096, 1, 1]) 16777216\n",
      "torch.Size([4096]) 4096\n",
      "torch.Size([2, 4096, 1, 1]) 8192\n",
      "torch.Size([2]) 2\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in pt_gender.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "# for m in pt_gender.named_modules():\n",
    "#   print(m[0])\n",
    "  \n",
    "for p in pt_gender.parameters():\n",
    "  print(p.shape, p.numel())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 145002878\n",
      "torch.Size([64, 3, 3, 3]) 1728\n",
      "torch.Size([64]) 64\n",
      "torch.Size([64, 64, 3, 3]) 36864\n",
      "torch.Size([64]) 64\n",
      "torch.Size([128, 64, 3, 3]) 73728\n",
      "torch.Size([128]) 128\n",
      "torch.Size([128, 128, 3, 3]) 147456\n",
      "torch.Size([128]) 128\n",
      "torch.Size([256, 128, 3, 3]) 294912\n",
      "torch.Size([256]) 256\n",
      "torch.Size([256, 256, 3, 3]) 589824\n",
      "torch.Size([256]) 256\n",
      "torch.Size([256, 256, 3, 3]) 589824\n",
      "torch.Size([256]) 256\n",
      "torch.Size([512, 256, 3, 3]) 1179648\n",
      "torch.Size([512]) 512\n",
      "torch.Size([512, 512, 3, 3]) 2359296\n",
      "torch.Size([512]) 512\n",
      "torch.Size([512, 512, 3, 3]) 2359296\n",
      "torch.Size([512]) 512\n",
      "torch.Size([512, 512, 3, 3]) 2359296\n",
      "torch.Size([512]) 512\n",
      "torch.Size([512, 512, 3, 3]) 2359296\n",
      "torch.Size([512]) 512\n",
      "torch.Size([512, 512, 3, 3]) 2359296\n",
      "torch.Size([512]) 512\n",
      "torch.Size([4096, 512, 7, 7]) 102760448\n",
      "torch.Size([4096]) 4096\n",
      "torch.Size([4096, 4096, 1, 1]) 16777216\n",
      "torch.Size([4096]) 4096\n",
      "torch.Size([2622, 4096, 1, 1]) 10739712\n",
      "torch.Size([2622]) 2622\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in torch_model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "# for m in torch_model.named_modules():\n",
    "#   print(m[0])\n",
    "for p in torch_model.parameters():\n",
    "  print(p.shape, p.numel())  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
